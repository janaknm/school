#+TITLE:     AI Notes
#+AUTHOR:    Matt Forbes
#+EMAIL:     ma@ttforbes.com
#+DATE:      2011-01-15 Sat
#+DESCRIPTION: 
#+KEYWORDS: 
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 

* Introduction - Chapter 1
** Summary
This chapter covered the background of AI and how it became what it is
today. It explored the various meanings of the term "AI", one of which
is the idea of a "rational agent," which is what it means in this
book. Following the definition of the term, we get a history lesson on
the components and events that led to AI becoming a field of
study. Finally, we see the transitions of AI through industry and
science.
* Intelligent Agents - Chapter 2
** Summary
This chapter focuses on the definition of an intelligent agent. An
agent is any entity capable of acting on its environment, and an
intelligent agent is one that makes decisions that maximize progress
towards some goal, even when the path to that goal is not clear. It
also discusses ways that we can judge the successfulness of actions
that such an agent performs. Agents must be designed for the specific
type of environment it will be acting on, and there are quite a few
important characteristics that change the decision-making processes of
the agent; such as how much of the environment the agent "sees,"
whether there are factors out of the agent's control, if it is
sequential or episodic, etc. Finally we learn about the general
structure of an agent program and the different types of agents. It
almost seems silly to distinguish between some of them because the
differences are very subtle. 
** Chapter Notes
*** PEAS
Performance Measure
Environment
Actuators
Sensors
*** Agent
Perceives its environment and performs an action based on what it
observes.
*** Agent Function
Given a percept acquired from the environment, return an action for
the agent to perform.
*** Agent Program
An agent program is the glue between an actual agent and an agent
function. This agent perceives the world and the program feeds these
percepts to the agent function, which returns an action that must be
handed to the agent to perform.
*** Rationality
When an agent system is stochastic (non-deterministic) there is an
element of randomness that makes it impossible to easily map an
environment state to an action. A rational agent will pick an action
for each state that brings it closer to whatever goals it has. When it
has multiple/conflicting goals, it should prioritize the most
important of them.
*** Autonomy
A simple agent function has a lookup table that matches environments
or states to a corresponding action. This table is static, and its
size grows exponentially with the number percepts observed. An
autonomous agent 'adapts' to its perceptions and learns. For this
reason, it doesn't need this infinitely large condition-action table.
*** Reflex Agent
These take an incoming percept and just match it to a predefined
action or combine it with the current state and match that state to an
action. Former of these two types is a simple reflex agent, but both
of them require an infinitely large lookup table.
*** Model-Based Agent
Calling an agent model-based would imply that it is keeping track of
hchanges in its environment by building up some 'state.' While the
structure of the state can vary, it is always built comparing previous
actions with the new percept. In this way, the agent function can find
cause-effect relationships between actions and changes in the world.
*** Goal-Based Agent
Some agents have a set of goals that may or may not be conflicting. A
goal-based agent tries to maximize the progress towards to these goals
with respect to their priority.
*** Utility-Based Agent
After each action an agent makes, its environment will most likely
change in some way. Based on the purpose of the agent, the new state
can be better or worse in terms of utility. When the state of the
environment can be mapped to a real number, there is a measure of
success of an action. If the utility is higher, we made a good move.
*** Learning Agent
Learning agent learns. An agent that learns will be much more
versatile in that it can become competent in environments it knows
nothing about to start with. These agents are composed of four
components: the learning element, performance element, the critic, and
the problem generator.

A learning element creates changes in actions and when to perform
them. It is told by the critic how the agent's actions are doing, as
well as getting feedback about how actions are changing the world from
the performance element. With this information, it can propose changes
in how actions are chosen and which actions are available.

Performance elements choose the actions based on incoming percepts and
the current state of the world. Changes to the performance element
originate from the learning element.

Critics simply evaluates the state of the world with respect to a
static performance standard. The critic decides how to categorize
events, such as dictating that check mates in chess are good, and
hitting neighboring cars in a taxi is bad. The standard must be fixed,
and the critic should be completely static; it can be seen as its own
agent altogether.

Without the problem generator, the agent would simply perform actions
determined by the performance element that were influenced by the
learning element. This setup could work fine, but introducing
creativity can lead to a better set of rules. A problem generator
proposes possible actions that may or may not be beneficial. Without
this, the agent may miss out on a lot of opportunities.

** Chapter Exercises
*** Problem 2.2
A performance measure describes how the current state of the
environment compares to the target environment. In a vaccuum model, a
performance measure could be the average number of squares that are
clean over a given interval. These are usually designed to express the
reason the agent exists. A vaccuum agent would exist to create clean
floors, not necessarily to suck dirt, so it wouldn't be a good measure
to compare number of times it sucked dirt.

Very similarly, there is the utility measure. Where the performance
measure evaluates how close the environment is to being at its target
state, a utility measure maps a state or sequence of states to a real
number. This number describes how well the course of events have
satisfied the agent's goals. It should consider conflicting goals and
priority when judging the effectiveness of the agent. 

*** Problem 2.3
There can be multiple agent programs that implement a given agent
function. For example, you could run an agent program on a computer in
order to run simulations. This would be different than implementing
the agent function in a program that runs on an agent that physically
performs the actions.

Any agent function can be implemented by a program. There might be
physical constraints, but hypothetically any function that maps
percepts and possibly a state can be implemented by a program.
*** Problem 2.5
**** Soccer player
Performance Measure: Goals made, steals, shots blocked, forward
progression of ball.

Environment: Soccer field.

Actuators: Legs, arms, head, body, mouth.

Sensors: Audio, Visual.

**** Internet book-shopping agent:
Performance Measure: Dollars saved on average, number of books bought
successfully, popularity of chosen books.

Environment: Internet.

Actuators: HTTP requests.

Sensors: HTTP responses.
**** Autonomous Mars rover
Performance Measure: Surface area discovered and mapped, unique
substances found.

Environment: Mars.

Actuators: Wheels, claws.

Sensors: Video, camera, microscope, other detection sensors.

**** Math theorem-proving assistant:
Performance Measure: Progress through proof, proofs solved, useful
hints offered, speed of solve.

Environment: Mathematician's office, computer.

Actuators: Screen display, audio.

Sensors: Keyboard input, web resources.

*** Problem 2.6
**** Soccer player
Partially Observable
Stochastic
Sequential
Dynamic
Continuous
Multiagent (cooperative and competitive)
**** Internet book-shopping agnet
Partially Observable
Stochastic
Episodic across multiple books. Sequential when looking for specific book
Static
Discrete
Single Agent
**** Autonomous mars rover
Partially Observable
Stochastic
Sequential
Dynamic
Continous
Single agent
**** Math theorem-proving assistant
Fully observable
Deterministic
Episodic for multiple proofs, sequential for current
Semidynamic
Discrete
Single Agent
** Programming
File is [[file:~/lisp/ai/exercises/2.7.lisp][here]].

* Uninformed Search - Chapter 3
** Summary
This chapter was mostly an introduction to methods of solving problems
by searching. It demonstrated how solutions to problems are much
easier to formulate when the problem is posed in a certain format,
namely when these components are defined: the initial state, possible
actions defined by a successor function, a goal test, and a path cost
which defines a numeric cost to each action. There are quite a few
different search techniques including breadth-first, depth-first,
bidrectional search, and variations of these. There are tradeoffs
between techniques, for example depth-first search uses much less
memory than breadth-first but may not be optimal. 
** Chapter Notes
*** state
A possible configuration of the environment.
*** state space
The set of all states.
*** search tree
A tree that describes possible sequences of states. The children of a
node are states that are accessible by performing some action. 
*** search node
These are the nodes that are in the search tree.  Each corresponds to
a certain state, and the action that was performed to get to it.
*** goal
When searching, the agent should generate sequences of states,
stopping when it's found one that ends in a goal state.
*** action
An action takes an agent from one state to another. Different actions
can map to different states.
*** successor function
Maps a state to a set of actions and resulting states that are
available. Used to build the children of a node in a search tree.
*** branching factor
Maximum number of results the given successor function can generate.
** Chapter Exercises
*** Problem 3.2
Goal formation must come before problem formation; the problem
dictates which actions and states should be considered in order to
accomplish something. If we don't have anything to shoot for then we
have no problem to solve.
*** Problem 3.3
legal-actions(s): denotes set of actions legal for state s
result(a,s): denotes state that results from performing a on s

vice;
successor(s): map( f(a): <a, result(a,s)> , legal-actions(s) )

versa;
legal-actions(s): map( f(x): first(x) , successor(s) )
result(a,s): first( lookup(a, successor(s)) )

* Informed (Heuristics) Search - Chapter 4
** Summary
This chapter highlights the problems with uninformed
search. Especially the case where the search space is so large, it
would be infeasible to search it for a solution. That's where
heuristics come in, they help guide the search process towards the
solution based on some auxillary function. For example, when searching
for the shortest route between two cities on a map, it is would be
better to check the edges that lead to cities closest to the
destination first. So, the heuristic in this example is the
straight-line distance. Using a heuristic leads to new types of
searches, most importantly though, is A*, which checks the nodes
estimated to be closest to the solution first. It makes estimates by
adding the path cost with the value of the heuristic
function. Finally, other informed search methods are introduced such
as hill climbing, beam search, and genetic algorithms.
* Knowledge Representation - Chapter 10
** Summary
** Chapter Notes
*** Ontological Engineering
Representing general concepts such as actions, time, physical objects,
and beliefs.
**** Upper Ontology
The general framework of concepts, rather than specific details.
*** Categories
Grouping of objects or other categories that define a set of truths or
attributes about its contained objects.
*** Disjoint Categories
Two groups are disjoin if they have no members in common
*** Exhaustive Decomposition
Say some category has n subcategories, then if there is an exhaustive
decomposition on these subcategories, then a member of the category
must also be one of the subcategories.
*** Partition
A partition is a disjoint exhaustive decomposition. Example: there is
a partition on the category animals over {male, female}. Every animal
must be either male or female, but never both.
*** Bunch
A way of representing a composite physical object of abstract
elements. We can't assign a weight to the category apples, because
"apples" is an abstract concept, but "bunch(apples)" is a composite
object that could possible have a weight.
*** Measurement Objects
Values assigned to properties by combining a units function with a
number, ie: Diameter(basketball) = Inches(9.5). Where "Inches" is a
units function taking a number.

Not all measures can be assigned a numeric value, such as beauty or
deliciousness, but they can be ranked nonetheless. It is important
that we can denote one measurement as "<" than another, even if their
values are not numerical.
*** Intrinsic Properties
Properties that belong to every instance of a certain object. When you
cut a substance in half, the two resulting pieces retain their
intrinsic values.
*** Extrinsic Properties
The opposite of intrinsic, these are properties like length or
weight. If you cut a basketball in half, they will not share the same
weight.
*** Mass Nouns (Substance)
Classes of objects that /only/ include intrinsic properties.
*** Count Nouns (Thing)
Any class of objects that include at least one extrinsic property.
*** Situation Calculus
Concentration on situations, which denote the states resulting from
executing actions, rather than explicit times.
*** Situations
Logical terms consisting of an initial situation S and all the
possible situations occurring after applying an action to S. So
Result(a, s) (sometimes called D) names the situation that results
when action a is executed in situation s.
*** Fluents
Functions and predicates that vary between situations, such as
location of the agent or inventory of the player. For example, the
fluent ~Holding(G, S) says that the agent is not holding the gold at
situation S.
*** Atemporal Predicates and Functions
These are "eternal"predicates and functions such as the function
LeftLegOf(Wumpus), or the predicate IsGold(g), which never change.
*** Projection
If an agent can deduce the outcome of a sequence of individual actions
on an initial state.
*** Planning
When an agent finds a suitable sequence of actions to achieve a
desired effect.
*** Possibility Axiom
Asking if an action is possible to be performed in a given situation.
*** Effect Axiom
If an action is performed in a given situation, what are the changes.
*** Frame Axioms
Say if a fluent stayed the same.
*** Frame Problem
Finding an efficient way to represent all the things in the world that
stay the same.
*** Successor-state Axiom
Describe how fluent predicates evolve over time. There is an if and
only if relationship between a fluent being true that depends solely
on the action and previous state. So the size of the axioms in a
representational frame problem is O(AE), because each of the E effects
of the A actions is mentioned in exactly one axiom.
*** Event Calculus
Provides a way to reason about periods of time instead of describing
fluents or situations. We can talk about an event happening, being
terminated, and clipped. An event is clipped between t1 and t2 if it
was terminated at some time between those times. A subevent is one
that is contained by some other, be it physically (location),
chronologically, or otherwise.
*** Event Categories
In the situation/fluent calculus models, an action would be something
like go(x, y), but in event calculus that would describe a category of
events. An object could go from x to y at an infinite number of times,
so it can not be an event itself, but a category of them. This
introduces specificity by reducing or incrementing arguments. For
example, go(x, y) is a subset of go(x), which describes all events
that move an object to the location x. 
*** Processes
Event categories such as flying(matt) that when broken apart into
subintervals and are still a member of the category are called
processes, or fluid event categories. If you take a short interval of
matt's flight from seattle to new york, that event is still in
flying(matt). The converse are discrete event categories that cannot
be broken up, for example fly(seattle, houston, new york) could not be
broken up because that would change the outcome.
*** E(c, i) and T(c, i)
E(c, i) denotes that a member of the event category c is a subevent of
the event/interval i.e. E( GotoClass(Matt, Networking), Tuesday). It
is true becuase matt went to networking class on tuesday.

T(c, i) means that some process was going on Throughout some interval,
rather than a subinterval. i.e. T( InClass(Matt, Networking),
TimeAM(10, 11)). 
*** States
Processes of continous nonchange. The state of matt being in
bellingham all day today would be denoted T(In(Matt, Bellingham), Today).




** Chapter Exercises
* Uncertainty - Chapter 13
** Maximum Expected Utility (MEU)
The action that yields the highest expected utility averaged over all
possible outcomes of tht action
** Atomic Events
An atomic event is a description of the entire world that an agent is
uncertain of; every random variable is set.
** Prior (Unconditional) Probability
The prior probability of a proposition p is the degree of belief that
it is true in the absense of ANY other information. It's the baseline
probability that a proposition is true.
** Conditional Probability
Denoted: P(a|b), representing the probability of a given only b.
** Diagnostic/Causal Information
Diagnostic info provides probabilities in the direction from symptoms
to causes. For example "A stiff neck implies meningitis in 1/5000
cases."

Causal knowledge should be independent of changes in the world (to a
point). This info deals with the way things fundamentally work, and is
much more robust than diagnostics.

** Bayes' Rule
This rule is really just an algebraic manipulation of the product
rule: P(a ^ b) = P(a | b)P(b) = P(b | a)P(a).

P(Y | X) = P(X | Y)P(Y) / P(X)
** Conditional Independence
When two variables become independent given the truth value of some
third value, we can simply the probability expression.

Let X and Y be indpendent given Z. Then we can simplify:

P(X,Y | Z) = P(X | Z)P(Y | Z)
and also:
P(X | Y,Z) = P(X | Z)

because X does not depend on Y when Z is given.

Conditional independence is much more common than absolute
independence (like "have cavity" is independence from "is
raining"). For a set of symptoms of some cause, when they are
conditionally independent the representation grows in O(n) rather than
O(2^n). Huge improvement.
* Concept Learning
** Concept
Single value function that returns positive for an instance that
satisfies the concept conditions.
** Instances
The set of items which a concept is defined are called instances,
denoted X.
** Target Concept
The concept to be learned, denoted c. ie. c : X --> {0,1}
** Training Examples
A list of instances for which we know the value of the target concept,
denoted D. x in D if c(x) is known, and x in X. <x, c(x)>
** Hypothesis
The goal of a concept learning is to find a function h, h : X -->
{0,1} such that h(x) = c(x) for x in X. All we know about the concept
is the training data, so our hypotheses are defined functions which we
can test against our known data.

